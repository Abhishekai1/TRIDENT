{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1f0d4e5485ef442c8a9def6dceca4d3e",
      "a05166c726fc4b7aa8deb3ed8cea830d",
      "709990d6a90e490f92caf47f7e6a637d",
      "6fe251ade9094f89960b26f19bdbb313",
      "3a0ff8f2b50e4cce934f33030a2a9343",
      "bcdaceeca1e047ee9722efa24b0874b8",
      "7b7a99d3675443359f86ddc642730e40",
      "f3d48b8998b84092ae4f09cf67908c50",
      "5ea71a8e394b4d748c86008d2f215669",
      "bce2ff4048eb4f49b92ccb1e4ad5edc2",
      "bc1c535e270443009920a79dbc51b7e4"
     ]
    },
    "id": "TPNUMpEXRBxT",
    "outputId": "6afbcfcb-9385-4706-95c8-da7e2920ee23"
   },
   "outputs": [],
   "source": [
    "# %% [setup]\n",
    "!pip -q install torch torchvision torchaudio transformers datasets sentence-transformers opacus scikit-learn matplotlib numpy pandas umap-learn shap\n",
    "\n",
    "import os, math, random, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances, roc_auc_score, precision_recall_curve, average_precision_score, confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from umap import UMAP\n",
    "from opacus import PrivacyEngine\n",
    "import shap\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rng = np.random.default_rng(42); torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "\n",
    "# %% [helpers]\n",
    "def set_seed(s=42):\n",
    "    rng = np.random.default_rng(s); torch.manual_seed(s); np.random.seed(s); random.seed(s)\n",
    "set_seed(42)\n",
    "\n",
    "def text_from_dataset(ds, prefer=(\"text\",\"title\",\"headline\",\"description\",\"content\")):\n",
    "    cols = set(ds.column_names)\n",
    "    for c in prefer:\n",
    "        if c in cols:\n",
    "            vals = ds[c]\n",
    "            return [v for v in vals if isinstance(v,str) and v.strip()]\n",
    "    sample = ds[0]\n",
    "    cand = [k for k,v in sample.items() if isinstance(v,str)]\n",
    "    if cand:\n",
    "        vals = ds[cand[0]]\n",
    "        return [v for v in vals if isinstance(v,str) and v.strip()]\n",
    "    raise ValueError(f\"No text-like columns. Columns: {ds.column_names}\")\n",
    "\n",
    "def plot_tradeoff(x, y, xlab, ylab, title):\n",
    "    plt.figure(); plt.plot(x, y, marker='o'); plt.xlabel(xlab); plt.ylabel(ylab); plt.title(title); plt.show()\n",
    "\n",
    "# %% [module A: robust news load + embeddings + echo auditor / reranker + visuals]\n",
    "try:\n",
    "    news = load_dataset(\"mteb/mind_small\")[\"train\"]\n",
    "    titles = text_from_dataset(news)\n",
    "    SRC = \"mteb/mind_small\"\n",
    "except Exception as e:\n",
    "    print(\"mteb/mind_small unavailable → using AG News:\", e)\n",
    "    news = load_dataset(\"wangrongsheng/ag_news\")[\"train\"]\n",
    "    titles = text_from_dataset(news, prefer=(\"text\",\"title\",\"description\",\"content\",\"headline\"))\n",
    "    SRC = \"wangrongsheng/ag_news\"\n",
    "print(\"News source:\", SRC, \"| items:\", len(titles))\n",
    "\n",
    "N = min(len(titles), 20000)\n",
    "titles = titles[:N]\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "X = embed_model.encode(titles, batch_size=256, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "K = 12\n",
    "km = KMeans(n_clusters=K, n_init=10, random_state=42).fit(X)\n",
    "clusters = km.labels_\n",
    "global_dist = np.bincount(clusters, minlength=K)/len(clusters)\n",
    "\n",
    "def simulate_users(U=300, hist_len=50, alpha=0.7):\n",
    "    prefs = rng.dirichlet(alpha*np.ones(K), size=U)\n",
    "    histories = []\n",
    "    for u in range(U):\n",
    "        probs = prefs[u]\n",
    "        p_idx = np.maximum(1e-12, probs[clusters]); p_idx /= p_idx.sum()\n",
    "        idxs = rng.choice(np.arange(N), size=hist_len, replace=False, p=p_idx)\n",
    "        histories.append(idxs)\n",
    "    return prefs, histories\n",
    "\n",
    "prefs, histories = simulate_users(U=300, hist_len=50, alpha=0.7)\n",
    "\n",
    "def user_vec(hist_idx): return np.mean(X[hist_idx], axis=0)\n",
    "\n",
    "def candidates(u_hist, C=120):\n",
    "    hot = rng.choice(u_hist, size=min(len(u_hist), 20), replace=False)\n",
    "    hot_cl = clusters[hot]\n",
    "    mask = np.isin(clusters, rng.choice(hot_cl, size=min(3,len(np.unique(hot_cl))), replace=False))\n",
    "    pool = np.where(mask)[0]\n",
    "    rest = np.setdiff1d(np.arange(N), pool)\n",
    "    c = np.unique(np.concatenate([\n",
    "        rng.choice(pool, size=min(len(pool), int(0.6*C)), replace=False) if len(pool)>0 else np.array([],dtype=int),\n",
    "        rng.choice(rest, size=min(len(rest), int(0.4*C)), replace=False)]))\n",
    "    return c[:C]\n",
    "\n",
    "def relevance(u_v, idx): return (X[idx] @ u_v)\n",
    "\n",
    "def xquad(cand_idx, rel, target_dist, lam=0.75, topk=50):\n",
    "    sel, cov = [], np.zeros(K)\n",
    "    R = set(cand_idx.tolist())\n",
    "    while R and len(sel)<topk:\n",
    "        best_i, best_s = None, -1e18\n",
    "        for i in R:\n",
    "            k = clusters[i]\n",
    "            cover = 1.0 - (cov[k] / max(1, target_dist[k]))\n",
    "            s = lam*rel[i] + (1-lam)*cover\n",
    "            if s>best_s: best_s, best_i = s, i\n",
    "        sel.append(best_i); cov[clusters[best_i]] += 1; R.remove(best_i)\n",
    "    return np.array(sel)\n",
    "\n",
    "def list_metrics(lst, u_v):\n",
    "    emb = X[lst]; d = pairwise_distances(emb, metric=\"cosine\")\n",
    "    ild = float(np.mean(d[np.triu_indices_from(d,1)]))\n",
    "    exp = np.bincount(clusters[lst], minlength=K); p = exp/np.sum(exp)\n",
    "    kl_g = float(np.sum(np.where(p>0, p*np.log(p/np.maximum(1e-12,global_dist)), 0.0)))\n",
    "    s = (X[lst] @ u_v); pr = 1/(1+np.exp(-4*s))\n",
    "    w = 1/np.log2(np.arange(1,len(lst)+1)+1)\n",
    "    clicks = float(np.sum(pr*w))\n",
    "    return ild, kl_g, clicks, exp\n",
    "\n",
    "def eval_user(u, lam=0.7):\n",
    "    h = histories[u]; u_v = user_vec(h)\n",
    "    cand_idx = candidates(h)\n",
    "    rel = np.zeros(N); rel[cand_idx] = relevance(u_v, cand_idx)\n",
    "    base = cand_idx[np.argsort(rel[cand_idx])[::-1]][:50]\n",
    "    hist_dist = np.bincount(clusters[h], minlength=K); hist_dist = hist_dist/np.sum(hist_dist)\n",
    "    target = 0.5*hist_dist + 0.5*global_dist\n",
    "    rer = xquad(cand_idx, rel, target, lam=lam, topk=50)\n",
    "    m_b = list_metrics(base, u_v); m_r = list_metrics(rer, u_v)\n",
    "    return m_b, m_r, base, rer\n",
    "\n",
    "U = len(histories)\n",
    "results = [eval_user(u, lam=0.7) for u in range(U)]\n",
    "ILD_b = np.array([r[0][0] for r in results]); KL_b = np.array([r[0][1] for r in results]); C_b = np.array([r[0][2] for r in results])\n",
    "ILD_r = np.array([r[1][0] for r in results]); KL_r = np.array([r[1][1] for r in results]); C_r = np.array([r[1][2] for r in results])\n",
    "\n",
    "print(\"Echo Auditor\")\n",
    "print(\"Baseline  : ILD=%.3f KL=%.3f Clicks=%.3f\"%(ILD_b.mean(), KL_b.mean(), C_b.mean()))\n",
    "print(\"Reranked  : ILD=%.3f KL=%.3f Clicks=%.3f\"%(ILD_r.mean(), KL_r.mean(), C_r.mean()))\n",
    "print(\"ΔILD=%.3f  ΔKL=%.3f  ΔClicks=%.3f\"%( (ILD_r-ILD_b).mean(), (KL_r-KL_b).mean(), (C_r-C_b).mean()))\n",
    "\n",
    "plt.figure(); plt.scatter(KL_b, KL_r, s=6); mx = float(max(KL_b.max(), KL_r.max())); plt.plot([0,mx],[0,mx])\n",
    "plt.xlabel(\"Baseline KL\"); plt.ylabel(\"Reranked KL\"); plt.title(\"KL to Global Dist per User\"); plt.show()\n",
    "\n",
    "umap = UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "emb2d = umap.fit_transform(X)\n",
    "plt.figure(); plt.scatter(emb2d[:,0], emb2d[:,1], c=clusters, s=3)\n",
    "plt.title(\"UMAP of News Embeddings (clusters)\"); plt.show()\n",
    "\n",
    "sample_u = 0\n",
    "_, _, base_s, rer_s = results[sample_u]\n",
    "exp_b = np.bincount(clusters[base_s], minlength=K); exp_r = np.bincount(clusters[rer_s], minlength=K)\n",
    "plt.figure(); plt.plot(exp_b/exp_b.sum(), marker='o', label=\"Baseline\"); plt.plot(exp_r/exp_r.sum(), marker='o', label=\"Reranked\"); plt.legend(); plt.title(\"Cluster Exposure (sample user)\"); plt.xlabel(\"Cluster\"); plt.ylabel(\"Share\"); plt.show()\n",
    "\n",
    "lams = [0.3,0.5,0.7,0.9]\n",
    "trade_ild, trade_clicks = [], []\n",
    "for L in lams:\n",
    "    vals = [eval_user(u, lam=L) for u in range(U)]\n",
    "    ild = np.mean([v[1][0] for v in vals]); clicks = np.mean([v[1][2] for v in vals])\n",
    "    trade_ild.append(ild); trade_clicks.append(clicks)\n",
    "plot_tradeoff(lams, trade_ild, \"λ\", \"ILD\", \"Diversity vs λ\")\n",
    "plot_tradeoff(lams, trade_clicks, \"λ\", \"Clicks (proxy)\", \"Relevance vs λ\")\n",
    "\n",
    "# %% [module B: DP phishing detector + conformal abstention + explainability]\n",
    "sms = load_dataset(\"ucirvine/sms_spam\")\n",
    "df = pd.DataFrame({\"text\": sms[\"train\"][\"sms\"], \"label\": [1 if t==\"spam\" else 0 for t in sms[\"train\"][\"label\"]]})\n",
    "tr, te = train_test_split(df, test_size=0.2, random_state=42, stratify=df.label)\n",
    "tr, ca = train_test_split(tr, test_size=0.1, random_state=42, stratify=tr.label)\n",
    "\n",
    "def embed_texts(txts, bs=256):\n",
    "    return embed_model.encode(txts, batch_size=bs, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)\n",
    "\n",
    "X_tr, y_tr = embed_texts(tr.text.tolist()), tr.label.values\n",
    "X_ca, y_ca = embed_texts(ca.text.tolist()), ca.label.values\n",
    "X_te, y_te = embed_texts(te.text.tolist()), te.label.values\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d=384, h=256): super().__init__(); self.f=nn.Sequential(nn.Linear(d,h),nn.ReLU(),nn.Linear(h,2))\n",
    "    def forward(self,x): return self.f(x)\n",
    "\n",
    "model = MLP().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "privacy_engine = PrivacyEngine()\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(torch.tensor(X_tr,dtype=torch.float32), torch.tensor(y_tr))), batch_size=256, shuffle=True, drop_last=True)\n",
    "model, opt, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model, optimizer=opt, data_loader=train_loader,\n",
    "    target_epsilon=8.0, target_delta=1e-5, epochs=6, max_grad_norm=1.0,\n",
    ")\n",
    "for _ in range(6):\n",
    "    model.train()\n",
    "    for xb,yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad(); loss = F.cross_entropy(model(xb), yb); loss.backward(); opt.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def probs(X):\n",
    "    t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    return F.softmax(model(t), dim=-1).cpu().numpy()\n",
    "\n",
    "p_te = probs(X_te); y_pred = p_te.argmax(1)\n",
    "auc = roc_auc_score(y_te, p_te[:,1]); ap = average_precision_score(y_te, p_te[:,1])\n",
    "print(\"DP Phishing: AUC=%.3f  AP=%.3f\"%(auc, ap))\n",
    "cm = confusion_matrix(y_te, (p_te[:,1]>=0.5).astype(int))\n",
    "print(\"Confusion matrix @0.5:\\n\", cm)\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_te, p_te[:,1])\n",
    "plt.figure(); plt.plot(rec, prec); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision-Recall\"); plt.show()\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_te, p_te[:,1], n_bins=10)\n",
    "plt.figure(); plt.plot(prob_pred, prob_true, marker='o'); plt.plot([0,1],[0,1],'--'); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Reliability Diagram\"); plt.show()\n",
    "\n",
    "def conformal_q(Xc, yc, alpha=0.05):\n",
    "    pc = probs(Xc); sc = 1 - pc[np.arange(len(yc)), yc]\n",
    "    return float(np.quantile(sc, 1-alpha, method=\"higher\"))\n",
    "alphas = [0.20,0.10,0.05,0.02]\n",
    "abst, sel_err, covg = [], [], []\n",
    "for a in alphas:\n",
    "    q = conformal_q(X_ca, y_ca, alpha=a)\n",
    "    sets = []\n",
    "    for p in p_te:\n",
    "        s = 1 - p.max()\n",
    "        sets.append([int(np.argmax(p))] if s<=q else [0,1])\n",
    "    covg.append(np.mean([y_te[i] in sets[i] for i in range(len(y_te))]))\n",
    "    idx = [i for i,s in enumerate(sets) if len(s)==1]\n",
    "    sel_err.append(1 - np.mean([int(y_pred[i]==y_te[i]) for i in idx]) if idx else np.nan)\n",
    "    abst.append(np.mean([len(s)==2 for s in sets]))\n",
    "plt.figure(); plt.plot(alphas, abst, marker='o'); plt.gca().invert_xaxis(); plt.xlabel(\"α\"); plt.ylabel(\"Abstention rate\"); plt.title(\"Abstention vs α\"); plt.show()\n",
    "plt.figure(); plt.plot(alphas, sel_err, marker='o'); plt.gca().invert_xaxis(); plt.xlabel(\"α\"); plt.ylabel(\"Error | non-abstain\"); plt.title(\"Selective Risk vs α\"); plt.show()\n",
    "\n",
    "# SHAP (Permutation) — fix: set max_evals >= 2*D+1 and add batch_size\n",
    "subset = min(100, len(X_te))\n",
    "D = X_te.shape[1]\n",
    "explainer = shap.Explainer(\n",
    "    lambda x: probs(x)[:,1],\n",
    "    X_te[:subset],\n",
    "    algorithm=\"permutation\",\n",
    "    max_evals=int(2*D + 1),\n",
    "    batch_size=50\n",
    ")\n",
    "shap_values = explainer(X_te[:subset])\n",
    "shap.summary_plot(shap_values, X_te[:subset], feature_names=[f\"f{i}\" for i in range(D)], show=False)\n",
    "plt.title(\"SHAP Summary (embedding features)\"); plt.show()\n",
    "\n",
    "# %% [module C: local DP telemetry for exposure audits + visuals]\n",
    "def randomized_response(cat, K, eps=2.0):\n",
    "    p = math.exp(eps)/(math.exp(eps)+K-1)\n",
    "    return cat if rng.random()<p else rng.choice([i for i in range(K) if i!=cat])\n",
    "\n",
    "user_exposures = []\n",
    "for h in histories:\n",
    "    dist = np.bincount(clusters[h], minlength=K).astype(float); dist/=dist.sum()\n",
    "    user_exposures.append(dist)\n",
    "user_exposures = np.array(user_exposures)\n",
    "true_pop = user_exposures.mean(0); true_pop/=true_pop.sum()\n",
    "\n",
    "eps_list = [0.5,1.0,2.0,4.0]\n",
    "l1_err = []\n",
    "for eps in eps_list:\n",
    "    noisy = [randomized_response(int(np.argmax(d)), K, eps) for d in user_exposures]\n",
    "    est = np.bincount(noisy, minlength=K)/len(noisy)\n",
    "    p = math.exp(eps)/(math.exp(eps)+K-1)\n",
    "    est = (est - (1-p)/K)/p; est = np.clip(est,0,1); est/=est.sum()\n",
    "    l1_err.append(float(np.abs(true_pop-est).sum()))\n",
    "plot_tradeoff(eps_list, l1_err, \"ε (privacy budget)\", \"L1 error\", \"LDP: Privacy vs Accuracy\")\n",
    "\n",
    "eps_show = 2.0\n",
    "noisy = [randomized_response(int(np.argmax(d)), K, eps_show) for d in user_exposures]\n",
    "est = np.bincount(noisy, minlength=K)/len(noisy)\n",
    "p = math.exp(eps_show)/(math.exp(eps_show)+K-1)\n",
    "est = (est - (1-p)/K)/p; est = np.clip(est,0,1); est/=est.sum()\n",
    "plt.figure(); plt.plot(true_pop, marker='o', label=\"True\"); plt.plot(est, marker='o', label=f\"LDP est (ε={eps_show})\"); plt.legend(); plt.xlabel(\"Cluster\"); plt.ylabel(\"Population share\"); plt.title(\"Population Exposure (Private Telemetry)\"); plt.show()\n",
    "\n",
    "# %% [summary]\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"News source: {SRC}, Items: {N}, Clusters: {K}, Users: {U}\")\n",
    "print(\"Echo Auditor: ILD (base→rerank) = %.3f → %.3f | KL = %.3f → %.3f | Clicks = %.3f → %.3f\" %\n",
    "      (ILD_b.mean(), ILD_r.mean(), KL_b.mean(), KL_r.mean(), C_b.mean(), C_r.mean()))\n",
    "print(\"Phishing (DP-SGD): AUC=%.3f, AP=%.3f\" % (auc, ap))\n",
    "print(\"Conformal: α\", alphas, \" | Abstention\", [round(x,3) for x in abst], \" | SelErr\", [None if np.isnan(x) else round(x,3) for x in sel_err], \" | Coverage\", [round(x,3) for x in covg])\n",
    "print(\"LDP telemetry: ε list\", eps_list, \" | L1 errors\", [round(x,3) for x in l1_err])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
